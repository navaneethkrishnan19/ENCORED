{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjFhCuldffLJ4UkACIP4X7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navaneethkrishnan19/ENCORED/blob/main/airlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxWADHaNxKi4",
        "outputId": "56b250a0-6785-4936-f6c0-89af8a8f0cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                          VirginAmerica dhepburn said\n",
            "1    VirginAmerica plus youve added commercials exp...\n",
            "2    VirginAmerica didnt today Must mean need take ...\n",
            "3    VirginAmerica really aggressive blast obnoxiou...\n",
            "4                   VirginAmerica really big bad thing\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset (assuming it's saved as 'Tweets.csv' in the current directory)\n",
        "twitter_df = pd.read_csv('Tweets.csv')\n",
        "\n",
        "# Sample text data\n",
        "text_data = twitter_df['text']\n",
        "\n",
        "# Define function for text cleaning and preprocessing\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Join tokens back into string\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply text cleaning and preprocessing to the text data\n",
        "cleaned_text_data = text_data.apply(clean_text)\n",
        "\n",
        "# Verify the result\n",
        "print(cleaned_text_data.head())\n"
      ]
    }
  ]
}